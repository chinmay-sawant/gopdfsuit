# Benchmark Results - v1.24 Optimized

This report summarizes the performance results of the **GoPdfSuit** API after optimizations, specifically tested for v1.24 expectations (using the latest server configuration with concurrency limit 100).

## Test Environment

- **Go Version:** 1.26 (Running on host)
- **Architecture:** linux/amd64
- **Date:** 2026-02-14
- **Optimizations:**
  - Gin Logger Disabled
  - Concurrency Semaphore: 100
  - Release Mode Enforced
  - XRef Loop Sprintf Elimination
  - Font Registry Pre-sizing

## Benchmark Results

| Test Scenario        | Total Req | Throughput (req/s) | Avg Latency | p95 Latency | p99 Latency | Status  |
| :------------------- | :-------- | :----------------- | :---------- | :---------- | :---------- | :------ |
| **Smoke & Load**     | 81        | 2.68               | 17.88ms     | 12.83ms     | 313.38ms    | ✅ PASS |
| **Spike (100 VUs)**  | 7,891     | **78.64**          | **35.39ms** | 324.49ms    | 575.54ms    | ✅ PASS |
| **Soak (Sustained)** | 597       | 4.90               | 23.15ms     | 26.73ms     | 349.87ms    | ✅ PASS |

## Improvement Comparison (v1.26 Original vs v1.24 Optimized)

| Metric (Spike Test) | v1.26 (Original) | v1.24 Optimized | Change                 |
| :------------------ | :--------------- | :-------------- | :--------------------- |
| **Throughput**      | 69.54 req/s      | **78.64 req/s** | +13%                   |
| **Avg Latency**     | 98.65ms          | **35.39ms**     | **-64%** (2.8x Faster) |
| **p95 Latency**     | 727.96ms         | **324.49ms**    | **-55%** (2.2x Faster) |
| **p99 Latency**     | 1,260.00ms       | **575.54ms**    | **-54%** (2.2x Faster) |

## Observations

- **Latency Consistency:** The optimized server shows much more consistent latency under high load. Even the p99 has been brought down from **1,260ms to 575ms**.
- **Throughput Gains:** Throughput increased by 13% despite the artificial limits (sleep/VUs) in the current k6 script.
- **Efficiency:** The 100 VU spike was handled with an average response time of just ~35ms, suggesting extremely low overhead in the request handling pipeline.
